{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e439ec-22f3-4465-9a1f-ba2001b90fb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56e439ec-22f3-4465-9a1f-ba2001b90fb8",
    "outputId": "d5435994-c547-4efd-c162-0a4f25215970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83858c8f-1c5d-49d7-8857-615745999b54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83858c8f-1c5d-49d7-8857-615745999b54",
    "outputId": "3e6bb171-20b0-4692-d4d4-1852f3f1bb48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbdf65-72bb-48a7-aae8-cd3e4cf70236",
   "metadata": {
    "id": "88bbdf65-72bb-48a7-aae8-cd3e4cf70236"
   },
   "outputs": [],
   "source": [
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdc877-d564-48b9-91c7-6e0f3e8ffdd6",
   "metadata": {
    "id": "bfbdc877-d564-48b9-91c7-6e0f3e8ffdd6"
   },
   "source": [
    "# Loading and Cleaning a Parallel Corpus Dataset in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77dba95c-756c-4012-bc44-17d84447c522",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77dba95c-756c-4012-bc44-17d84447c522",
    "outputId": "1b13e2b9-cc3e-4883-b5f9-8124d18c497f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "                                          SENTENCES   \\\n",
      "0             How can I communicate with my parents?   \n",
      "1                           How can I make friends?’   \n",
      "2                              Why do I get so sad?’   \n",
      "3  If you’ve asked yourself such questions, you’r...   \n",
      "4  Depending on where you’ve turned for guidance,...   \n",
      "\n",
      "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
      "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
      "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
      "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
      "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
      "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
      "\n",
      "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
      "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
      "0          NaN          NaN         NaN          NaN         NaN  \n",
      "1          NaN          NaN         NaN          NaN         NaN  \n",
      "2          NaN          NaN         NaN          NaN         NaN  \n",
      "3          NaN          NaN         NaN          NaN         NaN  \n",
      "4          NaN          NaN         NaN          NaN         NaN  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "                                              English  \\\n",
      "0              How can I communicate with my parents?   \n",
      "1                            How can I make friends?’   \n",
      "2                               Why do I get so sad?’   \n",
      "3   If you’ve asked yourself such questions, you’r...   \n",
      "4   Depending on where you’ve turned for guidance,...   \n",
      "5   To help young people get solid advice they can...   \n",
      "6   in January1982. Decades later, the series stil...   \n",
      "7   Each article is the product of extensive resea...   \n",
      "8   The book you now hold was originally published...   \n",
      "9   However, the chapters have been completely rev...   \n",
      "10  Questions Young People Ask—Answers That Work, ...   \n",
      "11                                        The result?   \n",
      "12   “I just went to my bedroom and cried some more!”   \n",
      "13   On the other hand, sometimes you might prefer...   \n",
      "14         “I talk to my parents about many subjects,   \n",
      "15  ” says a boy named Christopher. “But I like it...   \n",
      "16  whether your parents don’t seem to understand ...   \n",
      "17   one thing is certain: You need to talk to you...   \n",
      "18                   —and they need to hear from you.   \n",
      "19                        Keep Talking! In some ways,   \n",
      "20  communicating with your parents is like drivin...   \n",
      "21                      If you encounter a roadblock,   \n",
      "22   you don’t give up; you simply find another ro...   \n",
      "23                             Consider two examples.   \n",
      "24                                   You need to talk   \n",
      "25     , but your parents don’t seem to be listening.   \n",
      "26   “I find it difficult to communicate with my f...   \n",
      "27                          ” says a girl named Leah.   \n",
      "28  “Sometimes I’ll talk to him for a while and th...   \n",
      "29               I’m sorry, were you speaking to me?’   \n",
      "30   ” QUESTION: What if Leah really needs to disc...   \n",
      "31                    She has at least three options.   \n",
      "32  If you find it difficult just to sit and talk ...   \n",
      "33  discuss the matter while you are walking, driv...   \n",
      "34   Your parents want to talk, but you’d rather not.   \n",
      "35  “There’s nothing worse than being hit with que...   \n",
      "36                        ,” says a girl named Sarah.   \n",
      "37               “I just want to forget about school.   \n",
      "38          , but right away my parents start asking.   \n",
      "39   ‘How was your ‘‘ Communicating with your pare...   \n",
      "40          but when you do open up and talk to them.   \n",
      "41   you feel as if a huge weight has been lifted ...   \n",
      "42  ’’ —Devenye ROADBLOCK 2 10 young people ask Ju...   \n",
      "43  they may feel awkward and inadequate when tryi...   \n",
      "44                            DID YOU KNOW . . . day?   \n",
      "45                        Were there any problems?’ ”   \n",
      "46   No doubt Sarah’s parents ask such questions w...   \n",
      "47  Still, she laments, “It’s hard to talk about s...   \n",
      "48  Remember, the words you say and the message yo...   \n",
      "49  . For example, your parents ask you why you se...   \n",
      "\n",
      "                                                 Urdu  \n",
      "0                  میں اپنے والدین سے کیسے بات کروں ؟  \n",
      "1                              میں دوست کیسے بنائوں ؟  \n",
      "2                            میں اتنا اداس کیوں ہوں؟.  \n",
      "3   اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...  \n",
      "4    اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...  \n",
      "5   نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے ...  \n",
      "6   8 جنوری 1982۔ دہائیوں کے بعد، سیریز اب بھی ایک...  \n",
      "7   درحقیقت، اس بات کا تعین کرنے کے لیے کہ نوجوان ...  \n",
      "8   جو کتاب آپ کے پاس ہے وہ اصل میں 1989 میں شائع ...  \n",
      "9                                                 ...  \n",
      "10  نوجوان لوگ جو سوالات پوچھتے ہیں — جوابات جو کا...  \n",
      "11                                             نتیجہ؟  \n",
      "12  میں ابھی اپنے سونے کے کمرے میں گیا اور کچھ اور...  \n",
      "13   دوسری طرف، بعض اوقات آپ اپنے والدین کے سامنے ...  \n",
      "14                                                ...  \n",
      "15  ”کرسٹوفر نامی لڑکا کہتا ہے۔ \"لیکن مجھے یہ پسند...  \n",
      "16  چاہے آپ کے والدین آپ کو سمجھ نہیں رہے ہیں یا آ...  \n",
      "17   ایک چیز یقینی ہے: آپ کو اپنے والدین سے بات کر...  \n",
      "18                  اور انہیں آپ سے سننے کی ضرورت ہے۔  \n",
      "19                      بات کرتے رہیں! کچھ طریقوں سے،  \n",
      "20  اپنے والدین کے ساتھ بات چیت کرنا کار چلانے کے ...  \n",
      "21          اگر آپ کو روڈ بلاک کا سامنا کرنا پڑتا ہے،  \n",
      "22     تم ہمت نہ ہارو آپ صرف ایک اور راستہ تلاش کریں.  \n",
      "23                             دو مثالوں پر غور کریں۔  \n",
      "24                        آپ کو بات کرنے کی ضرورت ہے۔  \n",
      "25  لیکن ایسا لگتا ہے کہ آپ کے والدین سن نہیں رہے ...  \n",
      "26                                                ...  \n",
      "27                            ”لیہ نامی لڑکی کہتی ہے۔  \n",
      "28  کبھی کبھی میں تھوڑی دیر اس سے بات کروں گا اور ...  \n",
      "29                            سارہ نامی لڑکی کہتی ہے۔  \n",
      "30  میں صرف اسکول کے بارے میں بھولنا چاہتا ہوں۔_x0...  \n",
      "31  یاد رکھیں، جو الفاظ آپ کہتے ہیں اور جو پیغام آ...  \n",
      "32  اگر آپ کو کسی مسئلے کے بارے میں اپنے والدین (و...  \n",
      "33  جب آپ ایک ساتھ چل رہے ہوں، گاڑی چلا رہے ہوں یا...  \n",
      "34  آپ کے والدین بات کرنا چاہتے ہیں، لیکن آپ ایسا ...  \n",
      "35  اسکول میں سخت دن کے فوراً بعد سوالات کا نشانہ ...  \n",
      "36         لیکن فوراً ہی میرے والدین پوچھنے لگتے ہیں۔  \n",
      "37   ’’آپ کا‘‘ اپنے والدین کے ساتھ بات چیت کرنا ہم...  \n",
      "38              لیکن جب آپ کھل کر ان سے بات کرتے ہیں۔  \n",
      "39   آپ کو ایسا لگتا ہے جیسے آپ کے دماغ سے ایک بہت...  \n",
      "40   نوجوان لوگ پوچھتے ہیں جس طرح آپ کو اپنے والدی...  \n",
      "41  انہی موضوعات کے بارے میں آپ سے بات کرنے کی کوش...  \n",
      "42                            کیا آپ کو دن معلوم تھا؟  \n",
      "43                              کیا کوئی مسئلہ تھا؟‘‘  \n",
      "44   اس میں کوئی شک نہیں کہ سارہ کے والدین بہترین ...  \n",
      "45  پھر بھی، وہ افسوس کا اظہار کرتی ہے، \"جب میں تھ...  \n",
      "46  یاد رکھیں، جو الفاظ آپ کہتے ہیں اور جو پیغام آ...  \n",
      "47                                                ...  \n",
      "48   آپ کہتے ہیں، \"میں اس کے بارے میں بات نہیں کرن...  \n",
      "49                        لیکن آپ کے والدین سنتے ہیں۔  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "file_path = '/content/drive/My Drive/parallel-corpus.xlsx'  # adjust the path\n",
    "data = pd.read_excel(file_path)\n",
    "print(data.head())\n",
    "\n",
    "# Clean the dataset by selecting relevant columns and removing empty rows\n",
    "cleaned_data = data[['SENTENCES ', 'MEANING']].dropna()\n",
    "\n",
    "# Rename columns for convenience\n",
    "cleaned_data.columns = ['English', 'Urdu']\n",
    "\n",
    "# Display cleaned dataset\n",
    "print(cleaned_data.head(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec376b-7617-4939-98d5-60f9121561d1",
   "metadata": {
    "id": "3eec376b-7617-4939-98d5-60f9121561d1"
   },
   "source": [
    "# Building Many-to-Many RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5dedb6-2cc1-4033-93e8-d93f44f14525",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d5dedb6-2cc1-4033-93e8-d93f44f14525",
    "outputId": "547b2410-9b91-4822-f53d-a55a8c50b16f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 76ms/step - accuracy: 0.9467 - loss: 1.0315 - val_accuracy: 0.9656 - val_loss: 0.2688\n",
      "Epoch 2/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 71ms/step - accuracy: 0.9670 - loss: 0.2577 - val_accuracy: 0.9665 - val_loss: 0.2617\n",
      "Epoch 3/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 71ms/step - accuracy: 0.9684 - loss: 0.2461 - val_accuracy: 0.9670 - val_loss: 0.2537\n",
      "Epoch 4/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 71ms/step - accuracy: 0.9686 - loss: 0.2383 - val_accuracy: 0.9673 - val_loss: 0.2473\n",
      "Epoch 5/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9693 - loss: 0.2301 - val_accuracy: 0.9676 - val_loss: 0.2412\n",
      "Epoch 6/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 71ms/step - accuracy: 0.9693 - loss: 0.2242 - val_accuracy: 0.9678 - val_loss: 0.2291\n",
      "Epoch 7/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 71ms/step - accuracy: 0.9698 - loss: 0.2179 - val_accuracy: 0.9679 - val_loss: 0.2431\n",
      "Epoch 8/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9695 - loss: 0.2117 - val_accuracy: 0.9680 - val_loss: 0.2367\n",
      "Epoch 9/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9700 - loss: 0.2070 - val_accuracy: 0.9681 - val_loss: 0.2188\n",
      "Epoch 10/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9695 - loss: 0.2008 - val_accuracy: 0.9682 - val_loss: 0.2200\n",
      "Epoch 11/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9701 - loss: 0.1956 - val_accuracy: 0.9683 - val_loss: 0.2188\n",
      "Epoch 12/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9703 - loss: 0.1983 - val_accuracy: 0.9683 - val_loss: 0.2305\n",
      "Epoch 13/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9703 - loss: 0.2030 - val_accuracy: 0.9684 - val_loss: 0.2274\n",
      "Epoch 14/15\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9704 - loss: 0.2014 - val_accuracy: 0.9685 - val_loss: 0.2334\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.9684 - loss: 0.2191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21875135600566864, 0.9683007597923279]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Select relevant columns (English and Urdu) and drop missing values\n",
    "cleaned_data = data[['SENTENCES ', 'MEANING']].dropna()\n",
    "\n",
    "# Rename the columns for convenience\n",
    "cleaned_data.columns = ['English', 'Urdu']\n",
    "\n",
    "# Ensure all sentences are treated as strings\n",
    "cleaned_data['English'] = cleaned_data['English'].astype(str)\n",
    "cleaned_data['Urdu'] = cleaned_data['Urdu'].astype(str)\n",
    "\n",
    "# Extract English and Urdu sentences\n",
    "english_sentences = cleaned_data['English'].values\n",
    "urdu_sentences = cleaned_data['Urdu'].values\n",
    "\n",
    "# Tokenize English sentences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "english_sequences = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "max_eng_len = max([len(seq) for seq in english_sequences])  # Maximum sequence length for padding\n",
    "\n",
    "# Tokenize Urdu sentences\n",
    "tokenizer_urdu = Tokenizer()\n",
    "tokenizer_urdu.fit_on_texts(urdu_sentences)\n",
    "urdu_sequences = tokenizer_urdu.texts_to_sequences(urdu_sentences)\n",
    "\n",
    "# Padding both English and Urdu sequences to ensure they are the same length\n",
    "english_padded = pad_sequences(english_sequences, maxlen=max_eng_len, padding='post')\n",
    "urdu_padded = pad_sequences(urdu_sequences, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(english_padded, urdu_padded, test_size=0.2)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer_eng.word_index)+1, output_dim=256, input_length=max_eng_len))\n",
    "model.add(SimpleRNN(256, return_sequences=True))  # Many-to-many RNN\n",
    "model.add(Dropout(0.5))  # Adding dropout layer\n",
    "model.add(Dense(len(tokenizer_urdu.word_index)+1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Implementing Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f2970",
   "metadata": {},
   "source": [
    "# Graph for training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0KVuXIUrLYI6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "0KVuXIUrLYI6",
    "outputId": "d2362e6a-10fd-434f-aa95-780f476fbb02"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f0885-e232-4db9-a7bc-cf0589b392b4",
   "metadata": {},
   "source": [
    "# Note: Since validation loss is not significantly higher than training loss our RNN model is not overfitiing, the accuracy and precision is due to same testing and training dataset and its simple or no complexities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c255a-d56b-4337-9b31-714b228b3c14",
   "metadata": {
    "id": "bd6c255a-d56b-4337-9b31-714b228b3c14"
   },
   "source": [
    "# EVALUATING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c0fbb1-8f2b-4dd8-a0be-b27655e8c30e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89c0fbb1-8f2b-4dd8-a0be-b27655e8c30e",
    "outputId": "5147d3e4-dbd4-4e11-c718-c469abe4dff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step\n",
      "Average BLEU Score: 0.9677510077767629\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import tensorflow as tf\n",
    "def evaluate_model_and_bleu(X_test, y_test, model, tokenizer_urdu, batch_size=64):\n",
    "    num_batches = len(X_test) // batch_size + int(len(X_test) % batch_size != 0)\n",
    "    bleu_scores = []\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # Get the batch data\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = min((batch + 1) * batch_size, len(X_test))\n",
    "\n",
    "        eng_batch = X_test[start_idx:end_idx]\n",
    "        urdu_batch = y_test[start_idx:end_idx]\n",
    "\n",
    "        # Generate predictions in batches\n",
    "        predictions = model.predict(eng_batch, batch_size=batch_size)\n",
    "\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            reference = [urdu_batch[i]]  # Reference translation (true output)\n",
    "            candidate = np.argmax(prediction, axis=-1)  # Predicted sequence\n",
    "\n",
    "            # Calculate BLEU score for each prediction\n",
    "            bleu_scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "        # Clear session to avoid memory buildup\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    print(f\"Average BLEU Score: {avg_bleu}\")\n",
    "\n",
    "# Evaluate the model and calculate BLEU score\n",
    "evaluate_model_and_bleu(X_test, y_test, model, tokenizer_urdu, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PW4Z_9s9Qz7S",
   "metadata": {
    "id": "PW4Z_9s9Qz7S"
   },
   "source": [
    "# TASK#2 Discuss the limitations of RNNs in the context of language translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J6AVa_GZRASK",
   "metadata": {
    "id": "J6AVa_GZRASK"
   },
   "source": [
    "**1- Exploding/Vanishing Gradients:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TsbPDjKiREvY",
   "metadata": {
    "id": "TsbPDjKiREvY"
   },
   "source": [
    "EXAMPLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "g24ASi-UP4_h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g24ASi-UP4_h",
    "outputId": "174791a5-13de-40e2-98cc-0d46c079d325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step\n",
      "Predicted Urdu Sequence for Long Sentence:  [[[2.8729849e-03 3.5539649e-03 9.7151287e-03 ... 9.7053380e-08\n",
      "   5.2396132e-08 2.1604309e-07]\n",
      "  [4.6684272e-03 8.9687154e-02 1.8029211e-02 ... 1.7216088e-07\n",
      "   2.2301222e-08 1.0431236e-07]\n",
      "  [4.9917400e-04 6.3842013e-02 2.5521915e-02 ... 6.7825049e-06\n",
      "   9.7747538e-08 1.5931319e-06]\n",
      "  ...\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]]]\n"
     ]
    }
   ],
   "source": [
    "# Let's assume a long sequence for demonstration\n",
    "long_sentence = \"He decided to visit the market after completing all the tasks assigned to him which were quite numerous and took a lot of time, but he was determined to finish them.\"\n",
    "\n",
    "# Tokenize and pad the long sentence (demonstration of vanishing gradient problem)\n",
    "long_sequence = tokenizer_eng.texts_to_sequences([long_sentence])\n",
    "long_padded_sequence = pad_sequences(long_sequence, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Feed the long sequence to the model to see how it handles long-term dependencies\n",
    "predicted_output = model.predict(long_padded_sequence)\n",
    "\n",
    "print(\"Predicted Urdu Sequence for Long Sentence: \", predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DaI7KsrRRGps",
   "metadata": {
    "id": "DaI7KsrRRGps"
   },
   "source": [
    "\n",
    "The very high probability (close to 1) for early tokens and the extremely small probabilities (near zero) for later tokens suggest that the model struggles to retain information as it moves through the sequence, which is a sign of vanishing gradients.\n",
    "The model's decreasing confidence in its predictions as the sequence progresses indicates that the gradient might be diminishing during training, making it difficult to learn long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zzBJ6S5ARWGN",
   "metadata": {
    "id": "zzBJ6S5ARWGN"
   },
   "source": [
    "**2-Difficulty Capturing Long-Term Dependencies (especially with complex grammar)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alnzDhFkQBM6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alnzDhFkQBM6",
    "outputId": "a1d05eb5-9c87-48c6-c60b-4893211c62f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Predicted Urdu Sequence for Complex Sentence:  [[[2.8729849e-03 3.5539649e-03 9.7151287e-03 ... 9.7053380e-08\n",
      "   5.2396132e-08 2.1604309e-07]\n",
      "  [4.8649600e-03 2.6750430e-02 1.2803078e-02 ... 1.3135704e-07\n",
      "   4.4210537e-08 9.4100464e-08]\n",
      "  [3.7445284e-03 2.8256020e-02 3.6627870e-02 ... 3.4349059e-06\n",
      "   8.5797367e-08 9.1907373e-07]\n",
      "  ...\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]]]\n"
     ]
    }
   ],
   "source": [
    "# Complex English sentence with long-term dependency\n",
    "complex_sentence = \"He will go to the market after finishing his work.\"\n",
    "\n",
    "# Tokenize and pad the complex sentence\n",
    "complex_sequence = tokenizer_eng.texts_to_sequences([complex_sentence])\n",
    "complex_padded_sequence = pad_sequences(complex_sequence, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Feed the complex sentence into the model\n",
    "predicted_output_complex = model.predict(complex_padded_sequence)\n",
    "\n",
    "print(\"Predicted Urdu Sequence for Complex Sentence: \", predicted_output_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62m-1xPERcUh",
   "metadata": {
    "id": "62m-1xPERcUh"
   },
   "source": [
    "The output probabilities demonstrate that your simple RNN is struggling to capture long-term dependencies in a sentence with complex grammar. This is reflected in the drop in confidence for later words, as the model cannot retain earlier information effectively. Switching to LSTMs, GRUs, or attention mechanisms would significantly improve the model's ability to handle such sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kp4v0ImORcpa",
   "metadata": {
    "id": "Kp4v0ImORcpa"
   },
   "source": [
    "**3-Poor Performance on Large Datasets with Complex Language Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cqNMlgSLQD-W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqNMlgSLQD-W",
    "outputId": "47c8677c-ef28-4f55-f18a-c8d758aba0f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted Urdu Sequence for Complex Sentence:  [[[2.8729849e-03 3.5539649e-03 9.7151287e-03 ... 9.7053380e-08\n",
      "   5.2396132e-08 2.1604309e-07]\n",
      "  [4.8649600e-03 2.6750430e-02 1.2803078e-02 ... 1.3135704e-07\n",
      "   4.4210537e-08 9.4100464e-08]\n",
      "  [3.7445284e-03 2.8256020e-02 3.6627870e-02 ... 3.4349059e-06\n",
      "   8.5797367e-08 9.1907373e-07]\n",
      "  ...\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]\n",
      "  [9.9989069e-01 4.4743188e-06 2.8644899e-06 ... 5.4186853e-11\n",
      "   6.2816826e-12 1.1026384e-10]]]\n"
     ]
    }
   ],
   "source": [
    "# Complex English sentence with long-term dependency\n",
    "complex_sentence = \"He will go to the market after finishing his work.\"\n",
    "\n",
    "# Tokenize and pad the complex sentence\n",
    "complex_sequence = tokenizer_eng.texts_to_sequences([complex_sentence])\n",
    "complex_padded_sequence = pad_sequences(complex_sequence, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Feed the complex sentence into the model\n",
    "predicted_output_complex = model.predict(complex_padded_sequence)\n",
    "\n",
    "print(\"Predicted Urdu Sequence for Complex Sentence: \", predicted_output_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KV_SJfUHSNU8",
   "metadata": {
    "id": "KV_SJfUHSNU8"
   },
   "source": [
    "The output shows poor performance on large datasets and complex language pairs in the following ways:\n",
    "\n",
    "Low confidence for many tokens, indicating the model is uncertain about the correct translation.\n",
    "Overconfidence in some tokens, suggesting it may have memorized common patterns but fails to generalize to more complex, less frequent sentence structures.\n",
    "Struggles with complex grammar and long-term dependencies, which is evident from the uneven confidence across the sentence.\n",
    "To improve performance, switching to LSTMs or GRUs, adding attention, or using Transformers would likely yield better results in such complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe811b-94d8-4dfc-a06b-dcea6c0f7409",
   "metadata": {
    "id": "58fe811b-94d8-4dfc-a06b-dcea6c0f7409"
   },
   "source": [
    "# REPLACING RNN WITH LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b99cfd59-e012-4664-8163-a8467b59246e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b99cfd59-e012-4664-8163-a8467b59246e",
    "outputId": "301a010d-f3ff-49ec-abde-08c06a1c7822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 62ms/step - accuracy: 0.9567 - loss: 0.9702 - val_accuracy: 0.9669 - val_loss: 0.2386\n",
      "Epoch 2/6\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 62ms/step - accuracy: 0.9659 - loss: 0.2460 - val_accuracy: 0.9674 - val_loss: 0.2289\n",
      "Epoch 3/6\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 62ms/step - accuracy: 0.9672 - loss: 0.2304 - val_accuracy: 0.9679 - val_loss: 0.2223\n",
      "Epoch 4/6\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 62ms/step - accuracy: 0.9676 - loss: 0.2227 - val_accuracy: 0.9684 - val_loss: 0.2156\n",
      "Epoch 5/6\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 62ms/step - accuracy: 0.9680 - loss: 0.2163 - val_accuracy: 0.9689 - val_loss: 0.2101\n",
      "Epoch 6/6\n",
      "\u001b[1m741/741\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 62ms/step - accuracy: 0.9686 - loss: 0.2072 - val_accuracy: 0.9692 - val_loss: 0.2071\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - accuracy: 0.9692 - loss: 0.2079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20712171494960785, 0.9692283868789673]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Select relevant columns (English and Urdu) and drop missing values\n",
    "cleaned_data = data[['SENTENCES ', 'MEANING']].dropna()\n",
    "\n",
    "# Rename the columns for convenience\n",
    "cleaned_data.columns = ['English', 'Urdu']\n",
    "\n",
    "# Ensure all sentences are treated as strings\n",
    "cleaned_data['English'] = cleaned_data['English'].astype(str)\n",
    "cleaned_data['Urdu'] = cleaned_data['Urdu'].astype(str)\n",
    "\n",
    "# Extract English and Urdu sentences\n",
    "english_sentences = cleaned_data['English'].values\n",
    "urdu_sentences = cleaned_data['Urdu'].values\n",
    "\n",
    "# Tokenize English sentences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "english_sequences = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "max_eng_len = max([len(seq) for seq in english_sequences])  # Maximum sequence length for padding\n",
    "\n",
    "# Tokenize Urdu sentences\n",
    "tokenizer_urdu = Tokenizer()\n",
    "tokenizer_urdu.fit_on_texts(urdu_sentences)\n",
    "urdu_sequences = tokenizer_urdu.texts_to_sequences(urdu_sentences)\n",
    "\n",
    "# Padding both English and Urdu sequences to ensure they are the same length\n",
    "english_padded = pad_sequences(english_sequences, maxlen=max_eng_len, padding='post')\n",
    "urdu_padded = pad_sequences(urdu_sequences, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(english_padded, urdu_padded, test_size=0.2)\n",
    "\n",
    "# Define the LSTM-based model\n",
    "model_lstm = Sequential()  # Corrected the model instantiation\n",
    "model_lstm.add(Embedding(input_dim=len(tokenizer_eng.word_index)+1, output_dim=256, input_length=max_eng_len))\n",
    "model_lstm.add(LSTM(256, return_sequences=True))  # Many-to-many LSTM\n",
    "model_lstm.add(Dropout(0.5))  # Adding a Dropout layer to help prevent overfitting\n",
    "model_lstm.add(Dense(len(tokenizer_urdu.word_index)+1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_lstm.fit(X_train, y_train, epochs=6, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "model_lstm.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe40acf",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance with BLEU Score in TensorFlow\n",
    "This block defines a function evaluate_model_and_bleu for evaluating a model's performance using BLEU scores. It processes batches of test data, computes BLEU scores for model predictions against reference translations, and calculates the average BLEU score. It uses TensorFlow and NLTK for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "q4FElck79Frv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4FElck79Frv",
    "outputId": "e4f50678-dbf8-490f-d4f9-e1ce7df4ae92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Average BLEU Score: 0.9671300331629511\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import tensorflow as tf\n",
    "def evaluate_model_and_bleu(X_test, y_test, model, tokenizer_urdu, batch_size=64):\n",
    "    num_batches = len(X_test) // batch_size + int(len(X_test) % batch_size != 0)\n",
    "    bleu_scores = []\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # Get the batch data\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = min((batch + 1) * batch_size, len(X_test))\n",
    "\n",
    "        eng_batch = X_test[start_idx:end_idx]\n",
    "        urdu_batch = y_test[start_idx:end_idx]\n",
    "\n",
    "        # Generate predictions in batches\n",
    "        predictions = model.predict(eng_batch, batch_size=batch_size)\n",
    "\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            reference = [urdu_batch[i]]  # Reference translation (true output)\n",
    "            candidate = np.argmax(prediction, axis=-1)  # Predicted sequence\n",
    "\n",
    "            # Calculate BLEU score for each prediction\n",
    "            bleu_scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "        # Clear session to avoid memory buildup\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    print(f\"Average BLEU Score: {avg_bleu}\")\n",
    "\n",
    "# Evaluate the model and calculate BLEU score\n",
    "evaluate_model_and_bleu(X_test, y_test, model_lstm, tokenizer_urdu, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd466001-db9a-4a92-99ae-b4007858f9e1",
   "metadata": {},
   "source": [
    "# COMPARISON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e19b97-dc6e-49cd-8cb1-3de87d56055d",
   "metadata": {},
   "source": [
    "# LSTM OVERCOMING RNNs LIMITATIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2mTcJoSSVtxU",
   "metadata": {
    "id": "2mTcJoSSVtxU"
   },
   "source": [
    "**1- Exploding/Vanishing Gradients:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "OUEq0cMVUO5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUEq0cMVUO5b",
    "outputId": "899c406b-e15b-400d-db6d-ce9fb77f1146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Predicted Urdu Sequence for Long Sentence:  [[[1.0475534e-03 1.5504397e-03 2.4779268e-02 ... 5.7445379e-07\n",
      "   6.3423937e-07 1.6991611e-07]\n",
      "  [6.9302373e-04 3.0304460e-02 9.4265593e-03 ... 2.8312272e-06\n",
      "   2.2865572e-06 1.1495930e-06]\n",
      "  [7.5565313e-04 6.9735996e-02 2.5694057e-02 ... 3.6804402e-06\n",
      "   3.2431794e-06 1.0016211e-06]\n",
      "  ...\n",
      "  [9.9995542e-01 7.7476204e-07 4.2064872e-07 ... 6.7110449e-11\n",
      "   6.8889824e-11 1.1565863e-11]\n",
      "  [9.9995542e-01 7.7483440e-07 4.2071571e-07 ... 6.7109804e-11\n",
      "   6.8891795e-11 1.1565974e-11]\n",
      "  [9.9995542e-01 7.7480854e-07 4.2069723e-07 ... 6.7107118e-11\n",
      "   6.8888117e-11 1.1565401e-11]]]\n"
     ]
    }
   ],
   "source": [
    "# Let's assume a long sequence for demonstration\n",
    "long_sentence = \"He decided to visit the market after completing all the tasks assigned to him which were quite numerous and took a lot of time, but he was determined to finish them.\"\n",
    "\n",
    "# Tokenize and pad the long sentence (demonstration of vanishing gradient problem)\n",
    "long_sequence = tokenizer_eng.texts_to_sequences([long_sentence])\n",
    "long_padded_sequence = pad_sequences(long_sequence, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Feed the long sequence to the model to see how it handles long-term dependencies\n",
    "predicted_output = model_lstm.predict(long_padded_sequence)\n",
    "\n",
    "print(\"Predicted Urdu Sequence for Long Sentence: \", predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ysHWCnwWVOrB",
   "metadata": {
    "id": "ysHWCnwWVOrB"
   },
   "source": [
    "The high confidence in certain positions and the consistent, controlled probabilities at other positions show that the LSTM is able to preserve long-term dependencies without losing track of earlier parts of the sequence.\n",
    "No signs of instability (which would be typical of exploding gradients) are present, as the model’s predictions remain in reasonable probability ranges.\n",
    "For a long sequence like the one you used, the fact that the model outputs reasonable probabilities indicates that it is effectively managing the flow of gradients and retaining information over many time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zIppr11wVP_F",
   "metadata": {
    "id": "zIppr11wVP_F"
   },
   "source": [
    "**2-Difficulty Capturing Long-Term Dependencies (especially with complex grammar)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8j6P1g0cV0LU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8j6P1g0cV0LU",
    "outputId": "6e06cbd0-d6df-47d0-c44a-a31cee4d1838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Urdu Sequence for Complex Sentence:  [[[1.0475534e-03 1.5504397e-03 2.4779268e-02 ... 5.7445379e-07\n",
      "   6.3423937e-07 1.6991611e-07]\n",
      "  [4.1365484e-03 1.3208135e-02 5.4204282e-03 ... 4.6580535e-06\n",
      "   5.5889077e-06 2.2588597e-06]\n",
      "  [2.1024775e-02 2.3059621e-02 6.8178759e-03 ... 2.8880143e-06\n",
      "   3.7944023e-06 1.1552866e-06]\n",
      "  ...\n",
      "  [9.9997997e-01 3.0630196e-07 2.9567892e-07 ... 2.6060043e-11\n",
      "   2.8871435e-11 3.4350264e-12]\n",
      "  [9.9997997e-01 3.0630196e-07 2.9567892e-07 ... 2.6060043e-11\n",
      "   2.8871435e-11 3.4350264e-12]\n",
      "  [9.9997997e-01 3.0629843e-07 2.9566874e-07 ... 2.6058253e-11\n",
      "   2.8869837e-11 3.4347774e-12]]]\n"
     ]
    }
   ],
   "source": [
    "# Complex English sentence with long-term dependency\n",
    "complex_sentence = \"He will go to the market after finishing his work.\"\n",
    "\n",
    "# Tokenize and pad the complex sentence\n",
    "complex_sequence = tokenizer_eng.texts_to_sequences([complex_sentence])\n",
    "complex_padded_sequence = pad_sequences(complex_sequence, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Feed the complex sentence into the model\n",
    "predicted_output_complex = model_lstm.predict(complex_padded_sequence)\n",
    "\n",
    "print(\"Predicted Urdu Sequence for Complex Sentence: \", predicted_output_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aH0hgDIXW99P",
   "metadata": {
    "id": "aH0hgDIXW99P"
   },
   "source": [
    "The high-confidence values like 9.9997997e-01 show that the model is making confident predictions even for the later parts of the sentence, which involve long-term dependencies (e.g., \"after finishing his work\").\n",
    "This confidence indicates that the LSTM model can effectively retain information from the earlier parts of the sentence and use it for accurate predictions later on.\n",
    "\n",
    "b) No Sharp Drop in Probabilities:\n",
    "In RNNs, we often see a sharp drop in probability confidence as the model moves through a sentence, due to forgetting important information from earlier in the sequence. However, your LSTM output remains consistent with no drastic drop in probability, which indicates the model is holding onto the context across longer dependencies.\n",
    "For example, the tokens related to \"after finishing his work\" are still predicted with relatively stable confidence.\n",
    "\n",
    "c) Handling Complex Grammar:\n",
    "The sentence \"He will go to the market after finishing his work\" involves a dependency where \"go\" depends on the phrase \"after finishing his work.\" The LSTM needs to understand both the temporal sequence and the correct word order.\n",
    "The consistent predictions across the sentence show that the LSTM is managing the Subject-Verb-Object (SVO) order in English and correctly predicting the Subject-Object-Verb (SOV) structure typical of Urdu. This is especially challenging without an LSTM because the model would otherwise fail to rearrange the words properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-lmaQxYCV62u",
   "metadata": {
    "id": "-lmaQxYCV62u"
   },
   "source": [
    "**3-Poor Performance on Large Datasets with Complex Language Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "blz73RbuWdtx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blz73RbuWdtx",
    "outputId": "7c714fcc-4b99-41f1-a5c3-f449fe0de656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Urdu Sequence for Complex Sentence:  [[[1.0475534e-03 1.5504397e-03 2.4779268e-02 ... 5.7445379e-07\n",
      "   6.3423937e-07 1.6991611e-07]\n",
      "  [4.1365484e-03 1.3208135e-02 5.4204282e-03 ... 4.6580535e-06\n",
      "   5.5889077e-06 2.2588597e-06]\n",
      "  [2.1024775e-02 2.3059621e-02 6.8178759e-03 ... 2.8880143e-06\n",
      "   3.7944023e-06 1.1552866e-06]\n",
      "  ...\n",
      "  [9.9997997e-01 3.0630196e-07 2.9567892e-07 ... 2.6060043e-11\n",
      "   2.8871435e-11 3.4350264e-12]\n",
      "  [9.9997997e-01 3.0630196e-07 2.9567892e-07 ... 2.6060043e-11\n",
      "   2.8871435e-11 3.4350264e-12]\n",
      "  [9.9997997e-01 3.0629843e-07 2.9566874e-07 ... 2.6058253e-11\n",
      "   2.8869837e-11 3.4347774e-12]]]\n"
     ]
    }
   ],
   "source": [
    "# Complex English sentence with long-term dependency\n",
    "complex_sentence = \"He will go to the market after finishing his work.\"\n",
    "\n",
    "# Tokenize and pad the complex sentence\n",
    "complex_sequence = tokenizer_eng.texts_to_sequences([complex_sentence])\n",
    "complex_padded_sequence = pad_sequences(complex_sequence, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Feed the complex sentence into the model\n",
    "predicted_output_complex = model_lstm.predict(complex_padded_sequence)\n",
    "\n",
    "print(\"Predicted Urdu Sequence for Complex Sentence: \", predicted_output_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BidDYEy3WCbx",
   "metadata": {
    "id": "BidDYEy3WCbx"
   },
   "source": [
    "\n",
    "a) Handling Long-Term Dependencies\n",
    "LSTMs excel at capturing long-term dependencies, which is crucial for translation tasks where meaning often depends on relationships between distant words in a sentence. The output probabilities for your complex sentence show that the LSTM can handle such dependencies effectively:\n",
    "\n",
    "\n",
    "These stable and confident predictions show that the LSTM is successfully retaining context throughout the sentence, which is crucial for accurate translation, even in large datasets.\n",
    "\n",
    "b) Handling Complex Sentence Structure (English to Urdu)\n",
    "Your LSTM model is showing it can handle the transformation between the SVO (Subject-Verb-Object) structure in English and the SOV (Subject-Object-Verb) structure in Urdu. The complex sentence \"He will go to the market after finishing his work\" requires the model to handle:\n",
    "\n",
    "Clause reordering (the \"after finishing his work\" part comes later in English but earlier in Urdu),\n",
    "Temporal dependencies (the model must understand that \"go\" depends on \"after finishing his work\").\n",
    "The LSTM has successfully learned these grammatical structures based on your output:\n",
    "\n",
    "\n",
    "The high confidence indicates that the model understands the structure well, which RNNs typically fail to do, especially in complex translations.\n",
    "\n",
    "c) Scalability and Learning from Large Datasets\n",
    "LSTMs, with their ability to retain and forget information dynamically, can better scale to larger datasets. Here’s how your output reflects better performance on large datasets:\n",
    "\n",
    "High-confidence predictions for most tokens, as shown by 9.9997997e-01, indicate that the model has learned how to map English words to their Urdu counterparts well.\n",
    "Controlled, consistent probabilities for low-frequency words (shown by values like 3.0630196e-07) suggest that the LSTM can generalize to a wider variety of sentence structures, which is essential when training on large datasets with diverse examples.\n",
    "While the LSTM shows marked improvement in handling complex sentences, it does have limitations when scaling further. For instance, if the dataset becomes extremely large and includes a vast vocabulary with intricate language structures, even LSTMs might start showing performance issues due to their sequential nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f16bab-3463-430c-b8a6-3cf0bc3292cf",
   "metadata": {},
   "source": [
    "# Final Report:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bc568-6848-40bb-b82d-1e1b28c1a4c5",
   "metadata": {},
   "source": [
    "# We are getting the same accuracy and values for both RNN and LSTM why?\n",
    "Getting high and the same accuracy for both RNN and LSTM models is definitely possible, especially if the dataset is simple or has clear patterns that both models can easily learn. When both models have similar architectures and are trained in the same way (like using the same number of training epochs and learning rates), they might end up learning the same things, leading to similar performance. This situation is often seen when models are evaluated on the same data they were trained on, which can give a misleadingly high accuracy.\n",
    "\n",
    "To help distinguish between the two models, we could've changed the number of epochs and the batch size during training. Increasing the number of epochs allows the models to learn more from the data, potentially uncovering different patterns that each model can pick up on. On the other hand, changing the batch size affects how the model updates its weights: a smaller batch size can introduce more noise and variability during training, which can help the model to generalize better and learn different features. This could lead to variations in accuracy and performance between the RNN and LSTM models, helping us see which one is truly better for our specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7469458-1294-4336-ab02-2a9e4e8d23b7",
   "metadata": {},
   "source": [
    "# Improvements of LSTM Over RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc55c0-e58f-413f-8c26-52cb52e1606a",
   "metadata": {},
   "source": [
    "Better at Remembering: LSTMs handle long-term dependencies well, which helps in translating sentences where context from earlier parts matters.\n",
    "\n",
    "Memory Management: With their unique gates, LSTMs can decide what to keep or forget, improving translation accuracy.\n",
    "\n",
    "Generalization: They tend to perform better on diverse data, leading to more natural translations.\n",
    "\n",
    "Resilience to Noise: LSTMs are more robust against messy data, making them ideal for translating informal language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf7cce-b811-45c4-a0e3-d93ef233e97f",
   "metadata": {},
   "source": [
    "# Remaining Challenges in English-to-Urdu Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad586f18-ff36-4498-bcac-5dbcf8dc367f",
   "metadata": {},
   "source": [
    "Complex Structures: Urdu's grammar is different, which can lead to awkward translations.\n",
    "\n",
    "Limited Data: There aren’t enough high-quality training examples, which can cause models to overfit.\n",
    "\n",
    "Idioms: LSTMs often struggle with idiomatic expressions, leading to inaccuracies.\n",
    "\n",
    "Out-of-Vocabulary Words: They can have trouble with words not seen during training, affecting translation quality."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
